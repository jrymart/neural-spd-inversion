{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses a collection of [Landlab](https://landlab.csdms.io/)\n",
    "model runs. This notebook goes over a brief explanation of the model set\n",
    "up and parameters and then downloading and processing of the model runs\n",
    "from Zenodo.\n",
    "\n",
    "# The Model\n",
    "\n",
    "The model uses the following Landlab components:\n",
    "\n",
    "-   [LinearDiffuser](https://landlab.csdms.io/generated/api/landlab.components.diffusion.diffusion.html#landlab.components.diffusion.diffusion.LinearDiffuser)\n",
    "\n",
    "-   [FlowAccumulat](https://landlab.csdms.io/generated/api/landlab.components.flow_accum.flow_accumulator.html#landlab.components.flow_accum.flow_accumulator.FlowAccumulator)or\n",
    "\n",
    "-   [FastscapeEroder](https://landlab.csdms.io/generated/api/landlab.components.stream_power.fastscape_stream_power.html#landlab.components.stream_power.fastscape_stream_power.FastscapeEroder)\n",
    "\n",
    "    The model is implemented as subclass of the LandlabModel class to\n",
    "    allow for batch runs to be orchestrated by the [Landlab\n",
    "    Batcher](https://github.com/jrymart/landlab_batcher/tree/main)\n",
    "    utility. The code of the model is as follows:"
   ],
   "id": "fa21cae7-c4fc-4844-84b5-98d147bafdfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landlab.core import load_params\n",
    "from landlab.components import LinearDiffuser, FlowAccumulator, FastscapeEroder\n",
    "from model_base import LandlabModel\n",
    "import numpy as np\n",
    "\n",
    "class SimpleLem(LandlabModel):\n",
    "    def __init__(self, params={}):\n",
    "        \"\"\"Initialize the Model\"\"\"\n",
    "        super().__init__(params)\n",
    "\n",
    "        if not (\"topographic__elevation\" in self.grid.at_node.keys()):\n",
    "            self.grid.add_zeros(\"topographic__elevation\", at=\"node\")\n",
    "        rng = np.random.default_rng(seed=int(params[\"seed\"]))\n",
    "        grid_noise= rng.random(self.grid.number_of_nodes)/10\n",
    "        self.grid.at_node[\"topographic__elevation\"] += grid_noise\n",
    "        self.topo = self.grid.at_node[\"topographic__elevation\"]\n",
    "\n",
    "        self.uplift_rate = params[\"baselevel\"][\"uplift_rate\"]\n",
    "        self.diffuser = LinearDiffuser(\n",
    "            self.grid,\n",
    "            linear_diffusivity = params[\"diffuser\"][\"D\"]\n",
    "            )\n",
    "        self.accumulator = FlowAccumulator(self.grid, flow_director=\"D8\")\n",
    "        self.eroder = FastscapeEroder(self.grid,\n",
    "                                      K_sp=params[\"streampower\"][\"k\"],\n",
    "                                      m_sp=params[\"streampower\"][\"m\"],\n",
    "                                      n_sp=params[\"streampower\"][\"n\"],\n",
    "                                      threshold_sp=params[\"streampower\"][\"threshold\"])\n",
    "\n",
    "\n",
    "    def update(self, dt):\n",
    "        \"\"\"Advance the model by one time step of duration dt.\"\"\"\n",
    "        if self.current_time % 10000 == 0:\n",
    "            print(\"Model %s on year %d\" % (self.run_id, self.current_time))\n",
    "        self.topo[self.grid.core_nodes] += self.uplift_rate * dt\n",
    "        self.diffuser.run_one_step(dt)\n",
    "        self.accumulator.run_one_step()\n",
    "        self.eroder.run_one_step(dt)\n",
    "        self.current_time += dt\n"
   ],
   "id": "a15b9f62-7fac-4536-a40c-8e6ab9a6a5ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialized with random noise as elevation, an uplift rate,\n",
    "D8 flow router and accumulator and a diffusion and streampower erosion\n",
    "component with parameters defiend by the study. The parameters of this\n",
    "study are as follows:\n",
    "\n",
    "~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–+\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |\n",
    "|----|----|----|----|----|----|----|----|----|----|\n",
    "| Rows (cells) | Columns (cells) | Grid Spacing (m) | Runtime (years) | Timestep (years) | Uplift Rate () | Diffusivity ($\\frac{m^2}{year}$) | Streampower K ($\\frac{m^{0.4}}/year$) | Streampower m | Streampower n |\n",
    "\n",
    "~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–+\n",
    "\n",
    "|     |     |     |           |       |            |               |     |      |\n",
    "|-----|-----|-----|-----------|-------|------------|---------------|-----|------|\n",
    "| 300 | 100 | 5   | 3,000,000 | 0.001 | 0.005-0.02 | 0.00015-0.002 | 0.3 | 0.07 |\n",
    "\n",
    "~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–+ This parameter set was run\n",
    "with 10 different random seeds, and 30 linearly spaced values across the\n",
    "parameter range for a dataset of 9000 model runs. The final Landlab\n",
    "Grids of the model runs along with parameter information and associated\n",
    "database for the Landlab batcher utility, are available on Zenodo:\n",
    "<https://doi.org/10.5281/zenodo.15311644>.\n",
    "\n",
    "# Downloading Model Topography\n",
    "\n",
    "[Pooch](https://www.fatiando.org/pooch/latest/index.html) is used to\n",
    "download the data files from Zenodo. If you have previously downloaded\n",
    "the data pooch should not re-download."
   ],
   "id": "3b305de0-fc3d-41d5-a9d6-0e8ca1c03791"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pooch\n",
    "from config import DATA_PATH, NPY_URL, NPY_HASH, DB_URL, DB_HASH, MODEL_DEM_DIR\n",
    "\n",
    "DATA_PATH.mkdir(exist_ok=True, parents=True)\n",
    "model_dem_paths = pooch.retrieve(url=NPY_URL,\n",
    "                           known_hash=NPY_HASH,\n",
    "                           path=DATA_PATH,\n",
    "                           processor=pooch.Untar(extract_dir=MODEL_DEM_DIR))\n",
    "db_path = pooch.retrieve(url=DB_URL,\n",
    "                           known_hash=DB_HASH,\n",
    "                           fname=\"model_runs.db\",\n",
    "                           path=DATA_PATH)\n"
   ],
   "id": "975a4f0d-06a2-4f34-b40b-56e8212773d2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data for Flow Accumulation, Slope, and Curvature Rasters\n",
    "\n",
    "We generate flow accumulation roasters for each model DEM for later\n",
    "analysis"
   ],
   "id": "0f5bdcd3-bd4a-428b-86b6-8b5453c37794"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from landlab import RasterModelGrid\n",
    "from landlab.components import FlowAccumulator\n",
    "from config import FLOW_METHOD, MODEL_ACC_PATH, MODEL_LOG_ACC_PATH, MODEL_RESOLUTION, REPROCESS_DATA, MODEL_SLOPE_PATH, MODEL_CURV_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "for dem_path in model_dem_paths:\n",
    "    dem_path = Path(dem_path)\n",
    "    dem_id = dem_path.stem\n",
    "    slope_path = MODEL_SLOPE_PATH / f\"{dem_id}.npy\"\n",
    "    curvature_path = MODEL_CURV_PATH / f\"{dem_id}.npy\"\n",
    "    acc_path = MODEL_ACC_PATH / f\"{dem_id}.npy\"\n",
    "    log_acc_path = MODEL_LOG_ACC_PATH / f\"{dem_id}\".npy\n",
    "    if slope_path.exists() and curvature_path.exists() and acc_path.exists() and not REPROCESS_DATA:\n",
    "        continue\n",
    "    dem_array = np.load(dem_path)\n",
    "    if not acc_path.exists() or REPROCESS_DATA:\n",
    "        grid = RasterModelGrid(dem_array.shape, MODEL_RESOLUTION)\n",
    "        grid.add_field(\"node\", \"topographic__elevation\", dem_array, units=\"m\")\n",
    "        accumulator = FlowAccumulator(grid, flow_director = FLOW_METHOD)\n",
    "        accumulator.run_one_step()\n",
    "        # this step is to ensure consistency in processing with real data\n",
    "        flow_acc_array = grid.at_node[\"drainage_area\"].reshape(grid.shape)\n",
    "        np.save(acc_path, flow_acc_array)\n",
    "        log_acc_array = np.log10(flow_acc_array)\n",
    "        np.save(log_ac_path, log_acc_array)\n",
    "    if not (slope_path.exists() or curvature_path.exists()) or REPROCESS_DATA:\n",
    "        dz_dy, dz_dx = np.gradient(dem_array, MODEL_RESOLUTION)\n",
    "    if not slope_path.exists() or REPROCESS_DATA:\n",
    "        slope = np.sqrt(dz_dx**2+dz_dy**2)\n",
    "        np.save(slope_path, slope)\n",
    "    if not curvature_path.exists() or REPROCESS_DATA:\n",
    "        dz_xy, dz_xx = np.gradient(dz_dx, MODEL_RESOLUTION)\n",
    "        dz_yy, dz_yx = np.gradient(dz_dy, MODEL_RESOLUTION)\n",
    "        laplacian = dz_xx+dz_yy\n",
    "        np.save(curvature_path, laplacian)\n"
   ],
   "id": "a64a42bc-8744-4b4c-b3a6-350f8a1d0d81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics\n",
    "\n",
    "We calculate some simple data statistics which will be used for\n",
    "normalizing the data prior to training the neural networks. We calculate\n",
    "statistics from the portion of the dataset used in training, so as not\n",
    "to taint the dataset with statistical information from the testing set.\n",
    "While our data is drawn from one distribution, this is beind done as a\n",
    "best practice."
   ],
   "id": "fc885ec2-dc32-4574-84f7-72d68a15245c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_array_statistics(array_paths, crop):\n",
    "    array_total_sum = 0.0\n",
    "    array_total_sum_sq = 0.0\n",
    "    array_total_count = 0\n",
    "    for path in array_paths:\n",
    "        data_array=np.load(path)[crop:-crop,crop:-crop]\n",
    "        array_total_sum += np.sum(data_array)\n",
    "        array_total_sum_sq += np.sum(np.square(data_array))\n",
    "        array_total_count += data_array.size\n",
    "    array_mean = array_total_sum / array_total_count\n",
    "    variance = (array_total_sum_sq / array_total_count) - np.square(array_mean)\n",
    "    array_std = np.sqrt(variance)\n",
    "    return {'inputs_mean': array_mean, 'inputs_std': array_std}\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "from config import SPLIT_BY_FIELD, TRAINING_FRACTION, PARAM_TABLE, RUN_ID_FIELD, MODEL_DEM_PATH, MODEL_ARRAY_CROP, LABEL_QUERY, OUTPUTS_TABLE, MODEL_STATS_PATH, DB_PATH, RECALCULATE_STATS\n"
   ],
   "id": "3b2bd3ce-d72a-462b-8e02-2776c0d967f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a parameter in the model parameter database (in this case) the\n",
    "seed to split between train and test Datasets, so we need to connect to\n",
    "and query the database for runs."
   ],
   "id": "bc276215-ad41-4437-9d21-ab2e39c56c80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(DB_PATH)\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(f\"SELECT DISTINCT \\\"{SPLIT_BY_FIELD}\\\" FROM {PARAM_TABLE}\")\n",
    "categories = [r[0] for r in cursor.fetchall()]\n",
    "split = int((len(categories) * TRAINING_FRACTION))\n",
    "train_categories = categories[:split]\n",
    "train_filter = f\"\\\"{SPLIT_BY_FIELD}\\\" IN ({', '.join([str(c) for c in train_categories])})\"\n",
    "cursor.execute(f\"SELECT {RUN_ID_FIELD} FROM {PARAM_TABLE} WHERE {train_filter}\")\n",
    "train_run_ids = [r[0] for r in cursor.fetchall()]\n",
    "# TODO don't need need to join, can import path dirm fix names also\n",
    "train_dem_paths = [MODEL_DEM_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_acc_paths = [MODEL_ACC_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_slope_paths = [MODEL_SLOPE_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_curvature_paths = [MODEL_CURV_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_logacc_paths = [MODEL_LOG_ACC_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "if not RECALCULATE_STATS:\n",
    "    with open(MODEL_STATS_PATH, 'r') as f:\n",
    "        statistics = json.load(f)\n",
    "else:\n",
    "    statistics = {}\n",
    "data_types = {'dem': train_dem_paths, 'accumulation': train_acc_paths, 'log_accumulation': train_logacc_paths, slope': train_slope_paths, 'curvature': train_curvature_paths}\n",
    "for data_type in data_types.keys():\n",
    "    if RECALCULATE_STATS or data_type not in statistics:\n",
    "        statistics[data_type] = get_array_statistics(data_types[data_type], MODEL_ARRAY_CROP)\n"
   ],
   "id": "6ffa806e-bcb3-460f-a07f-42b5712c0b3c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the statistics of the labels we will train the network\n",
    "to infer."
   ],
   "id": "65bbb4bb-bc16-4db9-9de2-7421b96bf81c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "limit = connection.getlimit(sqlite3.SQLITE_LIMIT_VARIABLE_NUMBER)\n",
    "for i in range(0, len(train_run_ids), limit):\n",
    "    current_chunk_runs = train_run_ids[i:i+limit]\n",
    "    # placeholders are a safe way to programatically construct an SQL query\n",
    "    placeholders = ', '.join(['?']*len(current_chunk_runs))\n",
    "    cursor.execute(f\"{LABEL_QUERY} WHERE {RUN_ID_FIELD} IN ({placeholders})\", current_chunk_runs)\n",
    "    labels += [l[0] for l in cursor.fetchall()]\n",
    "labels = np.array(labels, dtype=np.float64)\n",
    "statistics['labels'] = {'labels_mean': np.mean(labels),\n",
    "                       'labels_std' : np.std(labels)}\n",
    "with open(MODEL_STATS_PATH, 'w') as f:\n",
    "    json.dump(statistics, f)\n"
   ],
   "id": "f01ca081-15f2-4dff-a8a9-d0f766abb69f"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
