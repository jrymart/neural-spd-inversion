{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses a collection of [Landlab](https://landlab.csdms.io/)\n",
    "model runs. This notebook goes over a brief explanation of the model set\n",
    "up and parameters and then downloading and processing of the model runs\n",
    "from Zenodo.\n",
    "\n",
    "# The Model\n",
    "\n",
    "The model uses the following Landlab components:\n",
    "\n",
    "-   [LinearDiffuser](https://landlab.csdms.io/generated/api/landlab.components.diffusion.diffusion.html#landlab.components.diffusion.diffusion.LinearDiffuser)\n",
    "\n",
    "-   [FlowAccumulat](https://landlab.csdms.io/generated/api/landlab.components.flow_accum.flow_accumulator.html#landlab.components.flow_accum.flow_accumulator.FlowAccumulator)or\n",
    "\n",
    "-   [FastscapeEroder](https://landlab.csdms.io/generated/api/landlab.components.stream_power.fastscape_stream_power.html#landlab.components.stream_power.fastscape_stream_power.FastscapeEroder)\n",
    "\n",
    "    The model is implemented as subclass of the LandlabModel class to\n",
    "    allow for batch runs to be orchestrated by the [Landlab\n",
    "    Batcher](https://github.com/jrymart/landlab_batcher/tree/main)\n",
    "    utility. The code of the model is as follows:\n",
    "\n",
    "    ``` python\n",
    "    from landlab.core import load_params\n",
    "    from landlab.components import LinearDiffuser, FlowAccumulator, FastscapeEroder\n",
    "    from model_base import LandlabModel\n",
    "    import numpy as np\n",
    "\n",
    "    class SimpleLem(LandlabModel):\n",
    "         def __init__(self, params={}):\n",
    "            \"\"\"Initialize the Model\"\"\"\n",
    "            super().__init__(params)\n",
    "\n",
    "            if not (\"topographic__elevation\" in self.grid.at_node.keys()):\n",
    "                self.grid.add_zeros(\"topographic__elevation\", at=\"node\")\n",
    "            rng = np.random.default_rng(seed=int(params[\"seed\"]))\n",
    "            grid_noise= rng.random(self.grid.number_of_nodes)/10\n",
    "            self.grid.at_node[\"topographic__elevation\"] += grid_noise\n",
    "            self.topo = self.grid.at_node[\"topographic__elevation\"]\n",
    "\n",
    "            self.uplift_rate = params[\"baselevel\"][\"uplift_rate\"]\n",
    "            self.diffuser = LinearDiffuser(\n",
    "                self.grid,\n",
    "                linear_diffusivity = params[\"diffuser\"][\"D\"]\n",
    "                )\n",
    "            self.accumulator = FlowAccumulator(self.grid, flow_director=\"D8\")\n",
    "            self.eroder = FastscapeEroder(self.grid,\n",
    "                                          K_sp=params[\"streampower\"][\"k\"],\n",
    "                                          m_sp=params[\"streampower\"][\"m\"],\n",
    "                                          n_sp=params[\"streampower\"][\"n\"],\n",
    "                                          threshold_sp=params[\"streampower\"][\"threshold\"])\n",
    "\n",
    "\n",
    "        def update(self, dt):\n",
    "            \"\"\"Advance the model by one time step of duration dt.\"\"\"\n",
    "            if self.current_time % 10000 == 0:\n",
    "                print(\"Model %s on year %d\" % (self.run_id, self.current_time))\n",
    "            self.topo[self.grid.core_nodes] += self.uplift_rate * dt\n",
    "            self.diffuser.run_one_step(dt)\n",
    "            self.accumulator.run_one_step()\n",
    "            self.eroder.run_one_step(dt)\n",
    "            self.current_time += dt\n",
    "    ```\n",
    "\n",
    "    The model is initialized with random noise as elevation, an uplift\n",
    "    rate, D8 flow router and accumulator and a diffusion and streampower\n",
    "    erosion component with parameters defiend by the study. The\n",
    "    parameters of this study are as follows:\n",
    "\n",
    "    |  |  |  |  |  |  |  |\n",
    "    |---|---|---|---|---|---|------------------------------------------------------|\n",
    "    | Rows (cells) \\| Columns (cells) \\| Grid Spacing (m) \\| Runtime (years) \\| Timestep (years) \\| Uplift Rate () \\| Diffusivity ($\\frac{m^2}{year}$) \\| Streampower K ($\\frac{m^{0.4}}/year$) \\| Streampower m \\| Streampower n |  |  |  |  |  |  |\n",
    "    | 300 | 100 | 5 \\| 3,000,000 \\| 0.001 |  |  |  | 0.005-0.02 \\| 0.00015-0.002 \\| 0.3 \\| 0.07 \\| |\n",
    "\n",
    "This parameter set was run with 10 different random seeds, and 30\n",
    "linearly spaced values across the parameter range for a dataset of 9000\n",
    "model runs. The final Landlab Grids of the model runs along with\n",
    "parameter information and associated database for the Landlab batcher\n",
    "utility, are available on Zenodo:\n",
    "<https://doi.org/10.5281/zenodo.15311644>.\n",
    "\n",
    "# Downloading Model Topography\n",
    "\n",
    "[Pooch](https://www.fatiando.org/pooch/latest/index.html) is used to\n",
    "download the data files from Zenodo\n",
    "\n",
    "``` python\n",
    "import pooch\n",
    "from config import DATA_PATH, NPY_URL, NPY_HASH, DB_URL, DB_HASH, MODEL_DEM_DIR\n",
    "\n",
    "DATA_PATH.mdkir(exists_ok=True, parents=True)\n",
    "model_dem_paths = pooch.retrieve(url=NPY_URL,\n",
    "                           known_hash=NPY_HASH,\n",
    "                           path=DATA_PATH,\n",
    "                           processor=pooch.Untar(extract_dir=MODEL_DEM_DIR))\n",
    "db_path = pooch.retrieve(url=DB_URL,\n",
    "                           known_hash=DB_HASH,\n",
    "                           path=DATA_PATH)\n",
    "```\n",
    "\n",
    "# Processing data for Flow Accumulation rasters\n",
    "\n",
    "We generate flow accumulation rasters for each model DEM for later\n",
    "analysis\n",
    "\n",
    "``` python\n",
    "import richdem as rd\n",
    "import numpy as np\n",
    "from config import FLOW_METHOD, MODEL_ACC_DIR\n",
    "\n",
    "acc_paths = []\n",
    "for dem_path in model_dem_paths:\n",
    "    dem_array = np.load(dem_path)\n",
    "    dem_id = os.path.splitext(os.path.split(dem_path)[1])[0]\n",
    "    dem = rd.rdarray(dem_array, no_data=-9999)\n",
    "    # this step is to ensure consistency in processing with real data\n",
    "    dem_filled = rd.FillDepressions(dem, epsilon=False, in_place=False)\n",
    "    flow_acc = rd.FlowAccumulation(dem_filled, method=FLOW_METHOD)\n",
    "    flow_acc_array = np.array(flow_acc)\n",
    "    acc_path = os.path,join(DATA_PATH, MODEL_ACC_DIR, f\"{dem_id}.npy)\n",
    "    acc_paths.append(acc_path)\n",
    "    np.save(acc_path, flow_acc_array)\n",
    "```\n",
    "\n",
    "# Data Statistics\n",
    "\n",
    "We calculate some simple data statistics which will be used for\n",
    "normalizing the data prior to training the neural networks. We calculate\n",
    "statistics from the portion of the dataset used in training, so as not\n",
    "to taint the dataset with statistical information from the testing set.\n",
    "While our data is drawn from one distribution, this is beind done as a\n",
    "best practice.\n",
    "\n",
    "``` python\n",
    "def get_array_statistics(array_paths, crop):\n",
    "    array_total_sum = 0.0\n",
    "    array_total_sum_sq = 0.0\n",
    "    array_total_count = 0\n",
    "    for path in array_paths:\n",
    "        data_array=np.load(path)[crop:-crop,crop:-crop]\n",
    "        array_total_sum += np.sum(data_array)\n",
    "        array_total_sum_sq += np.sum(np.square(data_array))\n",
    "        array_total_count += data_array.size\n",
    "    array_mean = inputs_total_sum / inputs_total_count\n",
    "    variance = (array_total_sum_sq / inputs_total_count) - np.square(inputs.mean)\n",
    "    array_std = np.sqrt(variance)\n",
    "    return {'inputs_mean': array_mean, 'inputs_std': array_std}\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "from config import SPLIT_BY_FIELD, TRAINING_FRACTION, PARAM_TABLE, RUN_ID_FIELD, MODEL_DEM_PATH, MODEL_ARRAY_CROP, LABEL_QUERY, OUTPUTS_TABLE, MODEL_STATS_PATH\n",
    "```\n",
    "\n",
    "We use a parameter in the model parameter database (in this case) the\n",
    "seed to split between train and test datasets, so we need to connect to\n",
    "and query the database for runs.\n",
    "\n",
    "``` python\n",
    "connection = sqlite3.connect(db_path)\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(f\"SELECT DISTINCT \\\"{SPLIT_BY_FIELD}\\\" FROM {PARAM_TABLE}\")\n",
    "categories = [r[0] for r in cursor.fetchall()]\n",
    "split = int((len(categories) * TRAINING_FRACTION))\n",
    "train_categories = categories[:split]\n",
    "train_filter = f\"\\\"{SPLIT_BY_FIELD}\\\" IN ({', '.join([str(c) for c in train_categories])})\"\n",
    "cursor.execute(f\"SELECT {RUN_ID_FIELD} FROM {PARAM_TABLE} WHERE {train_filter}\")\n",
    "model_run_ids = [r[0] for in cursor.fetchall()]\n",
    "train_paths = [os.path.join(MODEL_DEM_PATH, \"f{name}.npy\") for name in model_run_ids]\n",
    "acc_paths = [os.path.join(DATA_PATH, MODEL_ACC_PATH \"f{name}.npy\") for run model_run_ids]\n",
    "statistics = {}\n",
    "statistics['dem'] = get_array_statistics(train_dem_paths, MODEL_ARRAY_CROP)\n",
    "statistics['accumulation'] = get_array_statistics(train_acc_paths, MODEL_ARRAY_CROP)\n",
    "```\n",
    "\n",
    "Lastly, we need the statistics of the labels we will train the network\n",
    "to infer.\n",
    "\n",
    "``` python\n",
    "labels = []\n",
    "limit = connection.getlimit(sqlite3.SQLITE_LIMIT_VARIABLE_NUMBER)\n",
    "for i in range(0, len(model_run_ids), limit):\n",
    "    current_chunk_runs = model_run_ids[i:i+limit]\n",
    "    # placeholders are a safe way to programatically construct an SQL query\n",
    "    placeholders = ', '.join(['?']*len(current_chunk_runs))\n",
    "    cursor.execute(f\"SELECT {LABEL_QUERY} FROM {OUTPUTS_TABLE} WHERE {RUN_ID_FIELD} IN ({placeholders})\", current_chunk_runs)\n",
    "    labels += [l[0] for l in cursor.fetchall()]\n",
    "labels = np.array(labels, dtype=np.float64)\n",
    "statistcs['labels'] = {'labels_mean': np.mean(labels),\n",
    "                       'labels_std' : np.std(labels)}\n",
    "with open(MODEL_STATS_PATH, 'w') as f:\n",
    "    json.dump(stats, f)\n",
    "```"
   ],
   "id": "0cccea64-135c-441f-99e9-471bdfdcaf74"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
