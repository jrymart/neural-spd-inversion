{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run to prepare the colab environment for the\n",
    "training and analysis notebooks"
   ],
   "id": "caa63fb2-10d8-4757-9bb5-700c4892387f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jrymart/neural-spd-inversion.git\n",
    "%cd neural-spd-inversion\n",
    "\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "!pip install -q .\n",
    "!pip install -q ./lib/landlab_torch_tools\n"
   ],
   "id": "ee72a819-0ec5-4946-b173-aca38c5cb4a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data and analysis are saved to the colab instance which will be\n",
    "deleted automatically when the colab instance is closed. The files can\n",
    "be downloaded, or saved to google drive.\n",
    "\n",
    "This project uses a collection of [Landlab](https://landlab.csdms.io/)\n",
    "model runs. This notebook goes over a brief explanation of the model set\n",
    "up and parameters and then downloading and processing of the model runs\n",
    "from Zenodo.\n",
    "\n",
    "# The Model\n",
    "\n",
    "The model uses the following Landlab components:\n",
    "\n",
    "-   [LinearDiffuser](https://landlab.csdms.io/generated/api/landlab.components.diffusion.diffusion.html#landlab.components.diffusion.diffusion.LinearDiffuser)\n",
    "\n",
    "-   [FlowAccumulat](https://landlab.csdms.io/generated/api/landlab.components.flow_accum.flow_accumulator.html#landlab.components.flow_accum.flow_accumulator.FlowAccumulator)or\n",
    "\n",
    "-   [FastscapeEroder](https://landlab.csdms.io/generated/api/landlab.components.stream_power.fastscape_stream_power.html#landlab.components.stream_power.fastscape_stream_power.FastscapeEroder)\n",
    "\n",
    "    The model is implemented as subclass of the LandlabModel class to\n",
    "    allow for batch runs to be orchestrated by the [Landlab\n",
    "    Batcher](https://github.com/jrymart/landlab_batcher/tree/main)\n",
    "    utility. The code of the model is as follows:"
   ],
   "id": "2e222f40-5d7f-4169-826f-09f54bcc2ee3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landlab.core import load_params\n",
    "from landlab.components import LinearDiffuser, FlowAccumulator, FastscapeEroder\n",
    "from model_base import LandlabModel\n",
    "import numpy as np\n",
    "\n",
    "class SimpleLem(LandlabModel):\n",
    "    def __init__(self, params={}):\n",
    "        \"\"\"Initialize the Model\"\"\"\n",
    "        super().__init__(params)\n",
    "\n",
    "        if not (\"topographic__elevation\" in self.grid.at_node.keys()):\n",
    "            self.grid.add_zeros(\"topographic__elevation\", at=\"node\")\n",
    "        rng = np.random.default_rng(seed=int(params[\"seed\"]))\n",
    "        grid_noise= rng.random(self.grid.number_of_nodes)/10\n",
    "        self.grid.at_node[\"topographic__elevation\"] += grid_noise\n",
    "        self.topo = self.grid.at_node[\"topographic__elevation\"]\n",
    "\n",
    "        self.uplift_rate = params[\"baselevel\"][\"uplift_rate\"]\n",
    "        self.diffuser = LinearDiffuser(\n",
    "            self.grid,\n",
    "            linear_diffusivity = params[\"diffuser\"][\"D\"]\n",
    "            )\n",
    "        self.accumulator = FlowAccumulator(self.grid, flow_director=\"D8\")\n",
    "        self.eroder = FastscapeEroder(self.grid,\n",
    "                                      K_sp=params[\"streampower\"][\"k\"],\n",
    "                                      m_sp=params[\"streampower\"][\"m\"],\n",
    "                                      n_sp=params[\"streampower\"][\"n\"],\n",
    "                                      threshold_sp=params[\"streampower\"][\"threshold\"])\n",
    "\n",
    "\n",
    "    def update(self, dt):\n",
    "        \"\"\"Advance the model by one time step of duration dt.\"\"\"\n",
    "        if self.current_time % 10000 == 0:\n",
    "            print(\"Model %s on year %d\" % (self.run_id, self.current_time))\n",
    "        self.topo[self.grid.core_nodes] += self.uplift_rate * dt\n",
    "        self.diffuser.run_one_step(dt)\n",
    "        self.accumulator.run_one_step()\n",
    "        self.eroder.run_one_step(dt)\n",
    "        self.current_time += dt\n"
   ],
   "id": "00caffcb-e286-40e8-af10-3c8644f032e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialized with random noise as elevation, an uplift rate,\n",
    "D8 flow router and accumulator and a diffusion and streampower erosion\n",
    "component with parameters defiend by the study. The parameters of this\n",
    "study are as follows:\n",
    "\n",
    "~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–+\n",
    "\n",
    "|  |  |  |  |  |  |  |  |  |  |\n",
    "|----|----|----|----|----|----|----|----|----|----|\n",
    "| Rows (cells) | Columns (cells) | Grid Spacing (m) | Runtime (years) | Timestep (years) | Uplift Rate () | Diffusivity ($\\frac{m^2}{year}$) | Streampower K ($\\frac{m^{0.4}}/year$) | Streampower m | Streampower n |\n",
    "\n",
    "~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–+\n",
    "\n",
    "|     |     |     |           |       |            |               |     |      |\n",
    "|-----|-----|-----|-----------|-------|------------|---------------|-----|------|\n",
    "| 300 | 100 | 5   | 3,000,000 | 0.001 | 0.005-0.02 | 0.00015-0.002 | 0.3 | 0.07 |\n",
    "\n",
    "~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–~~—–+ This parameter set was run\n",
    "with 10 different random seeds, and 30 linearly spaced values across the\n",
    "parameter range for a dataset of 9000 model runs. The final Landlab\n",
    "Grids of the model runs along with parameter information and associated\n",
    "database for the Landlab batcher utility, are available on Zenodo:\n",
    "<https://doi.org/10.5281/zenodo.15311644>.\n",
    "\n",
    "# Downloading Model Topography\n",
    "\n",
    "[Pooch](https://www.fatiando.org/pooch/latest/index.html) is used to\n",
    "download the data files from Zenodo. If you have previously downloaded\n",
    "the data pooch should not re-download."
   ],
   "id": "50c2928c-f05f-43d2-8bb4-0faa8d7410ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pooch\n",
    "from config import DATA_PATH, NPY_URL, NPY_HASH, DB_URL, DB_HASH, MODEL_DEM_DIR\n",
    "\n",
    "DATA_PATH.mkdir(exist_ok=True, parents=True)\n",
    "model_dem_paths = pooch.retrieve(url=NPY_URL,\n",
    "                           known_hash=NPY_HASH,\n",
    "                           path=DATA_PATH,\n",
    "                           processor=pooch.Untar(extract_dir=MODEL_DEM_DIR))\n",
    "db_path = pooch.retrieve(url=DB_URL,\n",
    "                           known_hash=DB_HASH,\n",
    "                           fname=\"model_runs.db\",\n",
    "                           path=DATA_PATH)\n"
   ],
   "id": "c7063c1a-78cb-4123-8173-37f9b056d856"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data for Flow Accumulation, Slope, and Curvature Rasters\n",
    "\n",
    "We generate flow accumulation roasters for each model DEM for later\n",
    "analysis"
   ],
   "id": "61979b96-2eeb-495c-9b27-e9dc1299b93f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from landlab import RasterModelGrid\n",
    "from landlab.components import FlowAccumulator\n",
    "from config import FLOW_METHOD, MODEL_ACC_PATH, MODEL_LOG_ACC_PATH, MODEL_RESOLUTION, REPROCESS_DATA, MODEL_SLOPE_PATH, MODEL_CURV_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "for dem_path in model_dem_paths:\n",
    "    dem_path = Path(dem_path)\n",
    "    dem_id = dem_path.stem\n",
    "    slope_path = MODEL_SLOPE_PATH / f\"{dem_id}.npy\"\n",
    "    curvature_path = MODEL_CURV_PATH / f\"{dem_id}.npy\"\n",
    "    acc_path = MODEL_ACC_PATH / f\"{dem_id}.npy\"\n",
    "    log_acc_path = MODEL_LOG_ACC_PATH / f\"{dem_id}\".npy\n",
    "    if slope_path.exists() and curvature_path.exists() and acc_path.exists() and not REPROCESS_DATA:\n",
    "        continue\n",
    "    dem_array = np.load(dem_path)\n",
    "    if not acc_path.exists() or REPROCESS_DATA:\n",
    "        grid = RasterModelGrid(dem_array.shape, MODEL_RESOLUTION)\n",
    "        grid.add_field(\"node\", \"topographic__elevation\", dem_array, units=\"m\")\n",
    "        accumulator = FlowAccumulator(grid, flow_director = FLOW_METHOD)\n",
    "        accumulator.run_one_step()\n",
    "        # this step is to ensure consistency in processing with real data\n",
    "        flow_acc_array = grid.at_node[\"drainage_area\"].reshape(grid.shape)\n",
    "        np.save(acc_path, flow_acc_array)\n",
    "        log_acc_array = np.log10(flow_acc_array)\n",
    "        np.save(log_ac_path, log_acc_array)\n",
    "    if not (slope_path.exists() or curvature_path.exists()) or REPROCESS_DATA:\n",
    "        dz_dy, dz_dx = np.gradient(dem_array, MODEL_RESOLUTION)\n",
    "    if not slope_path.exists() or REPROCESS_DATA:\n",
    "        slope = np.sqrt(dz_dx**2+dz_dy**2)\n",
    "        np.save(slope_path, slope)\n",
    "    if not curvature_path.exists() or REPROCESS_DATA:\n",
    "        dz_xy, dz_xx = np.gradient(dz_dx, MODEL_RESOLUTION)\n",
    "        dz_yy, dz_yx = np.gradient(dz_dy, MODEL_RESOLUTION)\n",
    "        laplacian = dz_xx+dz_yy\n",
    "        np.save(curvature_path, laplacian)\n"
   ],
   "id": "07700d84-dcfb-4b9d-9736-08ca67b5cec8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics\n",
    "\n",
    "We calculate some simple data statistics which will be used for\n",
    "normalizing the data prior to training the neural networks. We calculate\n",
    "statistics from the portion of the dataset used in training, so as not\n",
    "to taint the dataset with statistical information from the testing set.\n",
    "While our data is drawn from one distribution, this is beind done as a\n",
    "best practice."
   ],
   "id": "5bf95b40-f552-4c8a-ab65-215aec11bbfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_array_statistics(array_paths, crop):\n",
    "    array_total_sum = 0.0\n",
    "    array_total_sum_sq = 0.0\n",
    "    array_total_count = 0\n",
    "    for path in array_paths:\n",
    "        data_array=np.load(path)[crop:-crop,crop:-crop]\n",
    "        array_total_sum += np.sum(data_array)\n",
    "        array_total_sum_sq += np.sum(np.square(data_array))\n",
    "        array_total_count += data_array.size\n",
    "    array_mean = array_total_sum / array_total_count\n",
    "    variance = (array_total_sum_sq / array_total_count) - np.square(array_mean)\n",
    "    array_std = np.sqrt(variance)\n",
    "    return {'inputs_mean': array_mean, 'inputs_std': array_std}\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "from config import SPLIT_BY_FIELD, TRAINING_FRACTION, PARAM_TABLE, RUN_ID_FIELD, MODEL_DEM_PATH, MODEL_ARRAY_CROP, LABEL_QUERY, OUTPUTS_TABLE, MODEL_STATS_PATH, DB_PATH, RECALCULATE_STATS\n"
   ],
   "id": "185c73e5-15b9-4585-95a0-48e9302493b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a parameter in the model parameter database (in this case) the\n",
    "seed to split between train and test Datasets, so we need to connect to\n",
    "and query the database for runs."
   ],
   "id": "4e07fd0c-a827-42a7-a149-00d0ce7fe75f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(DB_PATH)\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(f\"SELECT DISTINCT \\\"{SPLIT_BY_FIELD}\\\" FROM {PARAM_TABLE}\")\n",
    "categories = [r[0] for r in cursor.fetchall()]\n",
    "split = int((len(categories) * TRAINING_FRACTION))\n",
    "train_categories = categories[:split]\n",
    "train_filter = f\"\\\"{SPLIT_BY_FIELD}\\\" IN ({', '.join([str(c) for c in train_categories])})\"\n",
    "cursor.execute(f\"SELECT {RUN_ID_FIELD} FROM {PARAM_TABLE} WHERE {train_filter}\")\n",
    "train_run_ids = [r[0] for r in cursor.fetchall()]\n",
    "# TODO don't need need to join, can import path dirm fix names also\n",
    "train_dem_paths = [MODEL_DEM_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_acc_paths = [MODEL_ACC_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_slope_paths = [MODEL_SLOPE_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_curvature_paths = [MODEL_CURV_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "train_logacc_paths = [MODEL_LOG_ACC_PATH / f\"{name}.npy\" for name in train_run_ids]\n",
    "if not RECALCULATE_STATS:\n",
    "    with open(MODEL_STATS_PATH, 'r') as f:\n",
    "        statistics = json.load(f)\n",
    "else:\n",
    "    statistics = {}\n",
    "data_types = {'dem': train_dem_paths, 'accumulation': train_acc_paths, 'log_accumulation': train_logacc_paths, slope': train_slope_paths, 'curvature': train_curvature_paths}\n",
    "for data_type in data_types.keys():\n",
    "    if RECALCULATE_STATS or data_type not in statistics:\n",
    "        statistics[data_type] = get_array_statistics(data_types[data_type], MODEL_ARRAY_CROP)\n"
   ],
   "id": "3582bd3e-f3ae-4afe-827a-2cbd780b13aa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the statistics of the labels we will train the network\n",
    "to infer."
   ],
   "id": "f758998e-6947-455f-9169-181d1ac9ef0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "limit = connection.getlimit(sqlite3.SQLITE_LIMIT_VARIABLE_NUMBER)\n",
    "for i in range(0, len(train_run_ids), limit):\n",
    "    current_chunk_runs = train_run_ids[i:i+limit]\n",
    "    # placeholders are a safe way to programatically construct an SQL query\n",
    "    placeholders = ', '.join(['?']*len(current_chunk_runs))\n",
    "    cursor.execute(f\"{LABEL_QUERY} WHERE {RUN_ID_FIELD} IN ({placeholders})\", current_chunk_runs)\n",
    "    labels += [l[0] for l in cursor.fetchall()]\n",
    "labels = np.array(labels, dtype=np.float64)\n",
    "statistics['labels'] = {'labels_mean': np.mean(labels),\n",
    "                       'labels_std' : np.std(labels)}\n",
    "with open(MODEL_STATS_PATH, 'w') as f:\n",
    "    json.dump(statistics, f)\n"
   ],
   "id": "91f63b96-d280-4852-a483-155fb5844526"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks to Infer the Peclet Number\n",
    "\n",
    "This notebook trains convolutional neural networks to infer the D/K\n",
    "ratio of 2D streampower-diffusion landscape evolution models"
   ],
   "id": "bf4c0114-9652-4ce1-993d-c55ab82c59dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train_peclet_model import PecletModelTrainer\n",
    "from ThreeLayerCNNRegressor import ThreeLayerCNNRegressor, JumboThreeLayerCNNRegressor\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from config import MODEL_STATS_PATH, DB_PATH, MODEL_DEM_PATH, MODEL_SLOPE_PATH, MODEL_ACC_PATH, MODEL_CURV_PATH, LABEL_QUERY, WEIGHTS_PATH, NN_SEEDS, NUM_EPOCHS, LEARNING_RATE, RETRAIN_MODELS\n"
   ],
   "id": "4a279a6f-b42a-4325-b3f4-b7bbb1daf526"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import IN_COLAB MODEL_DIM_DIR, MODEL_ACC_DIR, MODEL_LOG_ACC_DIR, MODEL_SLOPE_DIR, MODEL_CURV_DIR, TAR_DIR\n",
    "from pathlib import Path\n",
    "if IN_COLAB:\n",
    "    !cp -r {DATA_PATH} /content/data\n",
    "    DATA_PATH = Path(\"/content/data\")\n",
    "    MODEL_DEM_PATH = DATA_PATH / MODEL_DEM_DIR / TAR_DIR\n",
    "    MODEL_ACC_PATH = DATA_PATH / MODEL_ACC_DIR\n",
    "    MODEL_SLOPE_PATH = DATA_PATH / MODEL_SLOPE_DIR\n",
    "    MODEL_CURV_PATH = DATA_PATH / MODEL_CURV_DIR\n",
    "    MODEL_LOG_ACC_PATH = DATA_PATH / MODEL_LOG_ACC_DIR\n"
   ],
   "id": "efb3acc6-47ff-4689-af5f-6e97b07b7df5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_STATS_PATH, 'r') as f:\n",
    "    statistics = json.load(f)\n",
    "retrain = False\n",
    "data_types = [{'type': 'dem',\n",
    "               'data_path': MODEL_DEM_PATH},\n",
    "              {'type': 'accumulation',\n",
    "               'data_path': MODEL_ACC_PATH},\n",
    "              {'type': 'log_accumulation',\n",
    "               'data_path': MODEL_LOG_ACC_PATH},\n",
    "              {'type': 'slope',\n",
    "               'data_path': MODEL_SLOPE_PATH},\n",
    "              {'type': 'curvature',\n",
    "               'data_path': MODEL_CURV_PATH}\n",
    "               ]\n"
   ],
   "id": "88378f20-10af-4137-b5ea-19c9f87fb88e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in NN_SEEDS:\n",
    "    for data in data_types:\n",
    "        torch.manual_seed(seed)\n",
    "        weights_path = WEIGHTS_PATH / f\"{data['type']}_{seed}_weights.pt\"\n",
    "        if not weights_path.exists() or RETRAIN_MODELS:\n",
    "            trainer = PecletModelTrainer(DB_PATH,\n",
    "                                        data['data_path'],\n",
    "                                        ThreeLayerCNNRegressor(),\n",
    "                                        LABEL_QUERY,\n",
    "                                        epochs = NUM_EPOCHS,\n",
    "                                        learning_rate = LEARNING_RATE,\n",
    "                                        **statistics[data['type']],\n",
    "                                        **statistics['labels'])\n",
    "            trainer.train()\n",
    "            trainer.save_weights(weights_path)\n",
    "        if data['type']=='dem':\n",
    "            for second_data in data_types:\n",
    "                weights_path = WEIGHTS_PATH / f\"dem_{second_data['type']}_{seed}_weights.pt\"\n",
    "                if not weights_path.exists() or RETRAIN_MODELS:\n",
    "                    inputs_mean = np.stack([v['inputs_mean'] for k,v in statistics.items() if k in ('dem', second_data['type'])])[:, np.newaxis, np.newaxis]\n",
    "                    inputs_std= np.stack([v['inputs_std'] for k,v in statistics.items() if k in ('dem', second_data['type'])])[:, np.newaxis, np.newaxis]\n",
    "                    trainer = PecletModelTrainer(DB_PATH,\n",
    "                                                 [data['data_path'], second_data['data_path']],\n",
    "                                                 ThreeLayerCNNRegressor(channels=2),\n",
    "                                                 LABEL_QUERY,\n",
    "                                                 epochs = NUM_EPOCHS,\n",
    "                                                 learning_rate = LEARNING_RATE,\n",
    "                                                 inputs_mean = inputs_mean,\n",
    "                                                 inputs_std = inputs_std,\n",
    "                                                 **statistics['labels']\n",
    "                                                 )\n"
   ],
   "id": "e25393cd-851c-47d3-af34-dfbd9c3ee601"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also train a \"jumbo\" model with larger layers."
   ],
   "id": "a4cf043c-f4ff-47f0-a412-442a338b894c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = WEIGHTS_dir / 'dem_jumbo_run_weights.pt'\n",
    "if not weights_path.exists() or RETRAIN_MODELS:\n",
    "    torch.manual_seed(NN_SEEDS[0])\n",
    "    trainer = PecletModelTrainer(DB_PATH,\n",
    "                                MODEL_DEM_PATH,\n",
    "                                JumboThreeLayerCNNRegressor,\n",
    "                                LABEL_QUERY,\n",
    "                                epochs = NUM_EPOCHS,\n",
    "                                learning_rate = LEARNING_RATE,\n",
    "                                **statistics['dem'],\n",
    "                                **statistics['labels'])\n",
    "    trainer.train()\n",
    "    trainer.save_weights(weights_path)\n"
   ],
   "id": "82267485-495f-4942-9189-8304a56d81dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*"
   ],
   "id": "f8555eda-950e-475c-8872-d34ef42478be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train_peclet_model import PecletModelTrainer\n",
    "from ThreeLayerCNNRegressor import ThreeLayerCNNRegressor, JumboThreeLayerCNNRegressor\n",
    "import json\n",
    "\n",
    "from config import MODEL_STATS_PATH, DB_PATH, MODEL_DEM_PATH, LABEL_QUERY, OUTPUTS_TABLE, WEIGHTS_PATH, NN_SEEDS, MODEL_ACC_PATH, RESULTS_PATH, NUM_EPOCHS, LEARNING_RATE\n"
   ],
   "id": "876e1e10-c627-4505-a6ea-0f6c5e84cbad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_STATS_PATH, 'r') as f:\n",
    "    statistics = json.load(f)\n",
    "retrain = False\n",
    "data_types = [{'type': 'dem',\n",
    "               'data_path': MODEL_DEM_PATH},\n",
    "              {'type': 'accumulation',\n",
    "               'data_path': MODEL_ACC_PATH},\n",
    "              {'type': 'slope',\n",
    "               'data_path': MODEL_SLOPE_PATH},\n",
    "              {'type': 'curvature',\n",
    "               'data_path': MODEL_CURV_PATH}\n",
    "               ]\n"
   ],
   "id": "19dda864-c64b-4532-a62a-854b5b521c67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in NN_SEEDS:\n",
    "    for data in data_types:\n",
    "        torch.manual_seed(seed)\n",
    "        weights_path = WEIGHTS_PATH / f\"{data['type']}_{seed}_weights.pt\"\n",
    "        if not os.path.exists(weights_path) or retrain:\n",
    "            trainer = PecletModelTrainer(DB_PATH,\n",
    "                                        data['data_path'],\n",
    "                                        ThreeLayerCNNRegressor(),\n",
    "                                        LABEL_QUERY,\n",
    "                                        epochs = NUM_EPOCHS,\n",
    "                                        learning_rate = LEARNING_RATE,\n",
    "                                        **statistics[data['type']],\n",
    "                                        **statistics['labels'])\n",
    "            trainer.load_weights(weights_path)\n",
    "            trainer.evaluate()\n",
    "            csv_path = RESULTS_DIR / f\"{data['type']}_{seed}_results.csv\")\n",
    "            trainer.test_df.to_csv(csv_path)\n",
    "\n",
    "        if data['type']=='dem':\n",
    "            for second_data in data_types:\n",
    "                weights_path = WEIGHTS_PATH / f\"dem_{second_data['type']}_{seed}_weights.pt\"\n",
    "                if not weights_path.exists() or retrain:\n",
    "                    inputs_mean = np.stack([v['inputs_mean'] for k,v in statistics.items() if k in ('dem', second_data['type'])])[:, np.newaxis, np.newaxis]\n",
    "                    inputs_std= np.stack([v['inputs_std'] for k,v in statistics.items() if k in ('dem', second_data['type'])])[:, np.newaxis, np.newaxis]\n",
    "                    trainer = PecletModelTrainer(DB_PATH,\n",
    "                                                 [data['data_path'], second_data['data_path']],\n",
    "                                                 ThreeLayerCNNRegressor(channels=2),\n",
    "                                                 LABEL_QUERY,\n",
    "                                                 epochs = NUM_EPOCHS,\n",
    "                                                 learning_rate = LEARNING_RATE,\n",
    "                                                 inputs_mean = inputs_mean,\n",
    "                                                 inputs_std = inputs_std,\n",
    "                                                 **statistics['labels']\n",
    "                                                 )\n",
    "                    trainer.load_weights(weights_path)\n",
    "                    trainer.evaluate()\n",
    "                    csv_path = RESULTS_DIR / f\"dem_{second_data['type']}_{seed}_results.csv}\"\n",
    "                    trainer.test_df.to_csv(csv_path)\n",
    "\n"
   ],
   "id": "c6bf7101-2167-4668-b291-47c5d37cffaf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also train a \"jumbo\" model with larger layers."
   ],
   "id": "3d2e93c0-5de5-4c0a-bba4-096524a65205"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = WEIGHTS_PATH / 'dem_jumbo_run_weights.pt'\n",
    "torch.manual_seed(NN_SEEDS[0])\n",
    "traier = PecletModelTrainer(DB_PATH,\n",
    "                            MODEL_DEM_PATH,\n",
    "                            JumboThreeLayerCNNRegressor,\n",
    "                            LABEL_QUERY,\n",
    "                            epochs = NUM_EPOCHS,\n",
    "                            learning_rate = LEARNING_RATE,\n",
    "                            **stats['dem'],\n",
    "                            **stats['labels'])\n",
    "trainer.load_weights(weights_path)\n",
    "trainer.evaluate()\n",
    "csv_path = RESULTS_DIR / 'dem_jumbo_run_results.pt')\n",
    "trainer.test_df.to_csv(csv_path)\n"
   ],
   "id": "8f2ae022-8824-4608-a411-10ca8d509340"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
